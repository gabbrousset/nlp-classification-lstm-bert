{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "9_Od2IXg71uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "D8zY8aX47qXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 551\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "id": "7RNO5xyo7rPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getDevice():\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device"
      ],
      "metadata": {
        "id": "jzwfzFFL7zsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = getDevice()"
      ],
      "metadata": {
        "id": "9Vfsf0YBSfIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Acquire and Pre-process the Web of Science Dataset"
      ],
      "metadata": {
        "id": "LH7Y8Ms87590"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"WebOfScienceDataset/WOS11967\"\n",
        "GLOVE_PATH = \"glove.6B/glove.6B.300d.txt\""
      ],
      "metadata": {
        "id": "Xq6GFxBB7-IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 200 # dynamically set later\n",
        "EMBED_DIM = 300     # we are using GloVe 300d\n",
        "MAX_VOCAB = 10000\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "iFTwi_UMSiP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    simple text cleaning:\n",
        "    - lowercase\n",
        "    - remove non-alphanumeric (keep spaces)\n",
        "    - collapse multiple spaces\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def load_data(data_dir):\n",
        "    \"\"\"\n",
        "    loads X, Y, and YL1 from the directory\n",
        "    \"\"\"\n",
        "    with open(os.path.join(data_dir, 'X.txt'), 'r', encoding='utf-8') as f:\n",
        "        texts = [clean_text(line) for line in f.readlines()]\n",
        "\n",
        "    with open(os.path.join(data_dir, 'Y.txt'), 'r', encoding='utf-8') as f:\n",
        "        y_sub = [int(line.strip()) for line in f.readlines()]\n",
        "\n",
        "    with open(os.path.join(data_dir, 'YL1.txt'), 'r', encoding='utf-8') as f:\n",
        "        y_domain = [int(line.strip()) for line in f.readlines()]\n",
        "\n",
        "    return texts, y_sub, y_domain\n",
        "\n",
        "def get_dynamic_max_len(texts, percentile=95):\n",
        "    \"\"\"\n",
        "    justification for MAX_SEQ_LEN:\n",
        "    calculates the length covering 'percentile'% of all documents\n",
        "    avoids wasting memory on padding outliers\n",
        "    \"\"\"\n",
        "    lengths = [len(t.split()) for t in texts]\n",
        "    limit = int(np.percentile(lengths, percentile))\n",
        "    print(f\"95th percentile length is {limit}. Mean is {int(np.mean(lengths))}.\")\n",
        "    return limit"
      ],
      "metadata": {
        "id": "fwYMojZq8Bqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(texts, max_words=MAX_VOCAB, min_freq=2):\n",
        "    \"\"\"\n",
        "    builds a dictionary mapping words to integer indices\n",
        "    \"\"\"\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(text.split())\n",
        "\n",
        "    # special tokens\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "    # add words that meet criteria\n",
        "    for word, count in word_counts.most_common(max_words - 2):\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def encode_texts(texts, vocab, max_len=MAX_SEQ_LEN):\n",
        "    \"\"\"\n",
        "    converts list of text strings to a Tensor of integer sequences\n",
        "    \"\"\"\n",
        "    tensor_data = []\n",
        "    unk_idx = vocab[\"<UNK>\"]\n",
        "    pad_idx = vocab[\"<PAD>\"]\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        # convert to indices with UNK fallback\n",
        "        seq = [vocab.get(t, unk_idx) for t in tokens]\n",
        "\n",
        "        if len(seq) < max_len:\n",
        "            # pad with 0\n",
        "            seq = seq + [pad_idx] * (max_len - len(seq))\n",
        "        else:\n",
        "            # truncate\n",
        "            seq = seq[:max_len]\n",
        "\n",
        "        tensor_data.append(seq)\n",
        "\n",
        "    return torch.tensor(tensor_data, dtype=torch.long)"
      ],
      "metadata": {
        "id": "zS76RwQN8C46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_matrix(path, vocab, embed_dim=EMBED_DIM):\n",
        "    \"\"\"\n",
        "    parse GloVe text file and returns a weight matrix for the specific vocab\n",
        "    uses memory efficient loading: checks vocab while reading the file\n",
        "    \"\"\"\n",
        "    weights = np.random.uniform(-0.25, 0.25, (len(vocab), embed_dim))\n",
        "\n",
        "    if \"<PAD>\" in vocab:\n",
        "        weights[vocab[\"<PAD>\"]] = 0\n",
        "\n",
        "    hits = 0\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "\n",
        "            if word in vocab:\n",
        "                # only parse if we need this word\n",
        "                vector = np.array(values[1:], dtype=float)\n",
        "\n",
        "                if len(vector) == embed_dim:\n",
        "                    weights[vocab[word]] = vector\n",
        "                    hits += 1\n",
        "\n",
        "    print(f\"GloVe loaded. Found {hits} / {len(vocab)} words.\")\n",
        "    return torch.tensor(weights, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "VwxYVYa18ELU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    texts, y_sub, y_domain = load_data(DATA_DIR)\n",
        "\n",
        "    dynamic_max_len = get_dynamic_max_len(texts)\n",
        "\n",
        "    vocab = build_vocab(texts)\n",
        "\n",
        "    # encode X (Inputs)\n",
        "    X_tensor = encode_texts(texts, vocab, max_len=dynamic_max_len)\n",
        "\n",
        "    # encode Y (Targets)\n",
        "    Y_sub_tensor = torch.tensor(y_sub, dtype=torch.long)\n",
        "    Y_domain_tensor = torch.tensor(y_domain, dtype=torch.long)\n",
        "\n",
        "    embedding_weights = load_glove_matrix(GLOVE_PATH, vocab)\n",
        "\n",
        "    # create TensorDataset\n",
        "    # since we have everything in RAM, TensorDataset is fastest\n",
        "    dataset = TensorDataset(X_tensor, Y_domain_tensor, Y_sub_tensor)\n",
        "\n",
        "    # Train/Val/Test Split (64/16/20)\n",
        "    test_size = int(0.2 * len(dataset))\n",
        "    remaining_size = len(dataset) - test_size\n",
        "    val_size = int(0.2 * remaining_size)\n",
        "    train_size = remaining_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    train_ldr = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_ldr = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_ldr = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_ldr, val_ldr, test_ldr, embedding_weights, vocab, train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "UMQfcCNM8Ks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Implement LSTM and BERT models"
      ],
      "metadata": {
        "id": "6Ifa5lKmSyAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, init_type):\n",
        "        super(CustomLSTMCell, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # we need weights for input (x) and hidden state (h) for all 4 gates:\n",
        "        # i (input), f (forget), g (cell), o (output)\n",
        "        # we can combine them into one big Linear layer for efficiency: 4 * hidden_size\n",
        "\n",
        "        # input: input_size -> Output: 4 * hidden_size\n",
        "        self.weight_ih = nn.Linear(input_size, 4 * hidden_size)\n",
        "        # input: hidden_size -> Output: 4 * hidden_size\n",
        "        self.weight_hh = nn.Linear(hidden_size, 4 * hidden_size)\n",
        "\n",
        "        # initialize weights (from what I researched Xavier is standard for Tanh activations)\n",
        "        if init_type == 'xavier':\n",
        "            nn.init.xavier_uniform_(self.weight_ih.weight)\n",
        "            nn.init.xavier_uniform_(self.weight_hh.weight)\n",
        "        elif init_type == 'zero':\n",
        "            nn.init.zeros_(self.weight_ih.weight)\n",
        "            nn.init.zeros_(self.weight_hh.weight)\n",
        "        elif init_type == 'random':\n",
        "            nn.init.normal_(self.weight_ih.weight, mean=0, std=0.01)\n",
        "            nn.init.normal_(self.weight_hh.weight, mean=0, std=0.01)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        x: (batch_size, input_size)\n",
        "        state: tuple (h_prev, c_prev)\n",
        "        \"\"\"\n",
        "        h_prev, c_prev = state\n",
        "\n",
        "        # computing all gates at once\n",
        "        gates = self.weight_ih(x) + self.weight_hh(h_prev)\n",
        "\n",
        "        # splitting into 4 chunks (input, forget, cell, output)\n",
        "        i_gate, f_gate, g_gate, o_gate = gates.chunk(4, 1)\n",
        "\n",
        "        # applying activations\n",
        "        i = torch.sigmoid(i_gate)\n",
        "        f = torch.sigmoid(f_gate)\n",
        "        g = torch.tanh(g_gate)\n",
        "        o = torch.sigmoid(o_gate)\n",
        "\n",
        "        # updating memory cell (c_t)\n",
        "        # \"forget what is old, add what is new\"\n",
        "        c_next = (f * c_prev) + (i * g)\n",
        "\n",
        "        # updating hidden state (h_t)\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next"
      ],
      "metadata": {
        "id": "PyqjDEYF8LrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, output_dim, hidden_size=512, embedding_matrix=None, dropout=0.5, init_type='xavier'):\n",
        "        super(CustomLSTMModel, self).__init__()\n",
        "        self.device = getDevice()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "            # freezing embeddings for a couple reasons: its faster, and might want to compare different pre-trained embeddings\n",
        "            # self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.lstm_cell = CustomLSTMCell(embed_dim, hidden_size, init_type)\n",
        "\n",
        "        # classifier\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        # embed: [batch_size, seq_len, embed_dim]\n",
        "        x_emb = self.embedding(x)\n",
        "\n",
        "        # init states (zeros)\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size).to(self.device)\n",
        "        c_t = torch.zeros(batch_size, self.hidden_size).to(self.device)\n",
        "\n",
        "        # loop through sequence\n",
        "        for t in range(seq_len):\n",
        "            x_t = x_emb[:, t, :] # [batch_size, embed_dim]\n",
        "            h_t, c_t = self.lstm_cell(x_t, (h_t, c_t))\n",
        "\n",
        "        # classify using last hidden state\n",
        "        out = self.dropout(h_t)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_ldr, val_ldr, epochs=10, lr=0.001, l1_lambda=0.0, weight_decay=0.0):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "        print(f\"\\ntraining LSTM (output dim: {self.output_dim}) for {epochs} epochs\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for X_batch, Y_domain, Y_sub in train_ldr:\n",
        "                X_batch = X_batch.to(self.device)\n",
        "                # target based on model output dimension\n",
        "                target = Y_domain.to(self.device) if self.output_dim == 7 else Y_sub.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self(X_batch)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                # l1 regularization\n",
        "                if l1_lambda > 0:\n",
        "                    l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
        "                    loss += l1_lambda * l1_norm\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "            val_acc = self.evaluate_acc(val_ldr)\n",
        "            val_loss = self.evaluate_loss(val_ldr, criterion)\n",
        "\n",
        "            history[\"val_acc\"].append(val_acc)\n",
        "            history[\"val_loss\"].append(val_loss)\n",
        "            print(f\"epoch {epoch+1}/{epochs} | loss: {total_loss/len(train_ldr):.4f} | val acc: {val_acc:.2f}%\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            X = X.to(self.device)\n",
        "            outputs = self(X)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "        return predictions\n",
        "\n",
        "    def evaluate_acc(self, data_loader):\n",
        "        self.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, Y_domain, Y_sub in data_loader:\n",
        "                X_batch = X_batch.to(self.device)\n",
        "                target = Y_domain.to(self.device) if self.output_dim == 7 else Y_sub.to(self.device)\n",
        "\n",
        "                preds = self.predict(X_batch)\n",
        "\n",
        "                correct += (preds == target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        return 100 * correct / total\n",
        "\n",
        "    def evaluate_loss(self, data_loader, criterion):\n",
        "        self.eval()\n",
        "        total_loss = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "             for X_batch, Y_domain, Y_sub in data_loader:\n",
        "                X_batch = X_batch.to(self.device)\n",
        "                target = Y_domain.to(self.device) if self.output_dim == 7 else Y_sub.to(self.device)\n",
        "\n",
        "                outputs = self(X_batch)\n",
        "\n",
        "                loss = criterion(outputs, target)\n",
        "                total_loss += loss.item() * target.size(0)\n",
        "                total += target.size(0)\n",
        "\n",
        "        return total_loss / total"
      ],
      "metadata": {
        "id": "Fno4CQat8M9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT-based text classifier using specific pre-trained weights\n",
        "    includes functionality for fine-tuning and attention visualization\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim, model_name = 'bert-base-uncased', dropout = 0.3):\n",
        "        super().__init__()\n",
        "        self.device = getDevice()\n",
        "\n",
        "        # load pre-trained BERT\n",
        "        self.bert = BertModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT outputs (last_hidden_state, pooler_output, hidden_states, attentions)\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # use the pooler_output (embedding of [CLS] token) for classification\n",
        "        pooler_output = outputs.pooler_output\n",
        "\n",
        "        pooled_output = self.dropout(pooler_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_attention_maps(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        extracts attention maps for visualization\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(input_ids=input_ids.to(self.device),\n",
        "                                attention_mask=attention_mask.to(self.device))\n",
        "        return outputs.attentions\n",
        "\n",
        "    def fit(self, train_ldr, val_ldr,\n",
        "            lr = 2e-5,\n",
        "            epochs = 3,\n",
        "            patience = 2,\n",
        "            weight_decay = 0.0,\n",
        "            l1_lambda = 0.0):\n",
        "        \"\"\"\n",
        "        fine-tunes BERT\n",
        "        - requires input_ids and attention_mask\n",
        "        - data loaders must yield (input_ids, attention_mask, labels)\n",
        "        \"\"\"\n",
        "        # AdamW handles weight_decay\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_steps = len(train_ldr) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=0,\n",
        "                                                    num_training_steps=total_steps)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        print(f\"starting BERT fine-tuning\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0\n",
        "            correct_train = 0\n",
        "            total_samples = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            for batch in train_ldr:\n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = batch[1].to(self.device)\n",
        "                b_labels = batch[2].to(self.device)\n",
        "\n",
        "                self.zero_grad()\n",
        "                logits = self.forward(b_input_ids, b_input_mask)\n",
        "                loss = criterion(logits, b_labels)\n",
        "\n",
        "                if l1_lambda > 0:\n",
        "                     l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
        "                     loss += l1_lambda * l1_norm\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)  # clips gradients to prevent explosion\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_train_loss += loss.item() * b_input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct_train += (preds == b_labels).sum().item()\n",
        "                total_samples += b_input_ids.size(0)\n",
        "\n",
        "            avg_train_loss = total_train_loss / total_samples\n",
        "            train_acc = correct_train / total_samples\n",
        "\n",
        "            # validation\n",
        "            val_loss, val_acc = self.evaluate(val_ldr, criterion)\n",
        "\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(f\"epoch {epoch+1}/{epochs} [{epoch_time:.1f}s] | \"\n",
        "                  f\"train loss: {avg_train_loss:.4f} acc: {train_acc:.4f} | \"\n",
        "                  f\"val loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n",
        "\n",
        "            history[\"train_loss\"].append(avg_train_loss)\n",
        "            history[\"val_loss\"].append(val_loss)\n",
        "            history[\"train_acc\"].append(train_acc)\n",
        "            history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(self.state_dict(), \"best_bert_model.pth\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "        self.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, val_ldr, criterion):\n",
        "        self.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_ldr:\n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = batch[1].to(self.device)\n",
        "                b_labels = batch[2].to(self.device)\n",
        "\n",
        "                logits = self.forward(b_input_ids, b_input_mask)\n",
        "                loss = criterion(logits, b_labels)\n",
        "\n",
        "                total_loss += loss.item() * b_input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == b_labels).sum().item()\n",
        "                total += b_input_ids.size(0)\n",
        "\n",
        "        return total_loss / total, correct / total"
      ],
      "metadata": {
        "id": "0G3AH1v2Vk8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Run experiments"
      ],
      "metadata": {
        "id": "dM4CXqe2gDv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_lstm(train_ds, val_ds, vocab_size, output_dim, batch_size=BATCH_SIZE, embedding_matrix=None, epochs=10, hidden_size=512, dropout=0.5, l1_lambda=0.0, weight_decay=0.0, lr=0.001, init_type='xavier'):\n",
        "\n",
        "    # DataLoader inside to handle batch_size variation\n",
        "    train_ldr = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_ldr = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # LSTM Classifier\n",
        "    model = CustomLSTMModel(\n",
        "        vocab_size,\n",
        "        embedding_matrix.shape[1],\n",
        "        output_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        dropout=dropout,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        init_type=init_type\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ldr, val_ldr,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        l1_lambda=l1_lambda,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # best validation accuracy from history\n",
        "    return max(history.get(\"val_acc\", [0]))"
      ],
      "metadata": {
        "id": "LYSJwJ3KgEfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bert(dropout, l1_lambda, weight_decay, batch_size, lr,\n",
        "                  train_ds, val_ds, output_dim):\n",
        "\n",
        "    # DataLoader inside to handle batch_size variation\n",
        "    train_ldr = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_ldr = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # BERT Classifier (Note: re-init model each time is computationally heavy)\n",
        "    model = BERTClassifier(output_dim, dropout=dropout).to(DEVICE)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ldr, val_ldr,\n",
        "        lr=lr,\n",
        "        epochs=2, # low epochs for search speed\n",
        "        patience=1,\n",
        "        weight_decay=weight_decay,\n",
        "        l1_lambda=l1_lambda\n",
        "    )\n",
        "\n",
        "    # best validation accuracy from history\n",
        "    return max(history.get(\"val_acc\", [0]))"
      ],
      "metadata": {
        "id": "P2NM-bDugGcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_test_data(all_texts, all_labels, test_indices, max_length=256):\n",
        "    \"\"\"\n",
        "    prepares a TensorDataset for the BERT test set using the shared indices\n",
        "    \"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    test_texts = [all_texts[i] for i in test_indices]\n",
        "    test_labels = [all_labels[i] for i in test_indices]\n",
        "\n",
        "    # Encode test data\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        test_encodings['input_ids'],\n",
        "        test_encodings['attention_mask'],\n",
        "        torch.tensor(test_labels)\n",
        "    )\n",
        "    test_ldr = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    return test_ldr, test_texts, test_labels, tokenizer"
      ],
      "metadata": {
        "id": "0HN2gKFugHo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_texts(model, data_loader, raw_texts):\n",
        "    \"\"\"\n",
        "    identifies one correctly predicted and one incorrectly predicted document index\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_idx = None\n",
        "    incorrect_idx = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            b_input_ids, b_input_mask, b_labels = [b.to(model.device) for b in batch]\n",
        "\n",
        "            # predict\n",
        "            logits = model(b_input_ids, b_input_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # find indices in the current batch where prediction matches/mismatches label\n",
        "            correct_mask = (preds == b_labels).cpu().numpy()\n",
        "            incorrect_mask = (preds != b_labels).cpu().numpy()\n",
        "\n",
        "            if correct_idx is None and correct_mask.any():\n",
        "                # get index within the batch, map to dataset index\n",
        "                local_idx = np.where(correct_mask)[0][0]\n",
        "                global_idx = data_loader.dataset.indices[data_loader.batch_size * data_loader.dataloader.batch_sampler.last_index + local_idx]\n",
        "                correct_idx = global_idx\n",
        "\n",
        "            if incorrect_idx is None and incorrect_mask.any():\n",
        "                local_idx = np.where(incorrect_mask)[0][0]\n",
        "                global_idx = data_loader.dataset.indices[data_loader.batch_size * data_loader.dataloader.batch_sampler.last_index + local_idx]\n",
        "                incorrect_idx = global_idx\n",
        "\n",
        "            if correct_idx is not None and incorrect_idx is not None:\n",
        "                break\n",
        "\n",
        "    if correct_idx is None or incorrect_idx is None:\n",
        "        # fallback if the full data isn't used or model is too accurate/inaccurate\n",
        "        return raw_texts[0], raw_texts[-1], 0, 0\n",
        "\n",
        "    return raw_texts[correct_idx], raw_texts[incorrect_idx], correct_idx, incorrect_idx"
      ],
      "metadata": {
        "id": "srJSLZQigI_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_combined_results(values, model_1_scores, model_2_scores, title, xlabel, label_model_1=\"LSTM Val Acc\", label_model_2=\"BERT Val Acc\", save_dir=\"plots\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.plot(values, model_1_scores, label=label_model_1, marker='o')\n",
        "    ax.plot(values, model_2_scores, label=label_model_2, marker='x')\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(\"Validation Accuracy\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    clean_title = title.replace(\" \", \"_\").replace(\":\", \"\")\n",
        "    filename = f\"{clean_title}.png\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    fig.savefig(filepath, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"saved plot to: {filepath}\")"
      ],
      "metadata": {
        "id": "72MwGko2gKQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_single_result(values, scores, title, xlabel, ylabel=\"Validation Accuracy\", save_dir=\"plots\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.plot(values, scores, marker='o', label='LSTM')\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    clean_title = title.replace(\" \", \"_\").replace(\":\", \"\")\n",
        "    filename = f\"{clean_title}.png\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    fig.savefig(filepath, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"saved plot to: {filepath}\")"
      ],
      "metadata": {
        "id": "FkfoOXcqgLiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"loading data\")\n",
        "train_loader, val_loader, test_loader, embedding_matrix, vocab, train_ds, val_ds, test_ds = prepare_data()\n",
        "\n",
        "# reload raw texts for BERT (needed since LSTM used indices)\n",
        "all_texts, y_sub_raw, y_domain_raw = load_data(DATA_DIR)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = embedding_matrix.shape[1]\n",
        "output_dim_domain = 7\n",
        "output_dim_sub = 33"
      ],
      "metadata": {
        "id": "Zk8TLjDLgMpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indices to accurately map labels to raw text (for BERT's tokenizer)\n",
        "train_indices = train_ds.indices\n",
        "val_indices = val_ds.indices\n",
        "test_indices = test_ds.indices\n",
        "\n",
        "bert_train_labels = [y_domain_raw[i] for i in train_indices]\n",
        "bert_val_labels = [y_domain_raw[i] for i in val_indices]\n",
        "\n",
        "bert_sub_train_labels = [y_sub_raw[i] for i in train_indices]\n",
        "bert_sub_val_labels = [y_sub_raw[i] for i in val_indices]\n",
        "\n",
        "# tokenise the raw text for BERT separately\n",
        "bert_train_texts = [all_texts[i] for i in train_indices]\n",
        "bert_val_texts = [all_texts[i] for i in val_indices]\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "wcAJg68mgNh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) test dropout for LSTM (both classification tasks)\n",
        "dropout_list = [0.1, 0.3, 0.5]\n",
        "lstm_res_domain = []\n",
        "lstm_res_sub = []\n",
        "\n",
        "for d in dropout_list:\n",
        "    print(f\"testing dropout: {d}\")\n",
        "\n",
        "    l_acc_domain = evaluate_lstm(train_ds, val_ds, vocab_size, output_dim_domain, embedding_matrix=embedding_matrix, dropout=d)\n",
        "    lstm_res_domain.append(l_acc_domain)\n",
        "\n",
        "    l_acc_sub = evaluate_lstm(train_ds, val_ds, vocab_size, output_dim_sub, embedding_matrix=embedding_matrix, dropout=d, epochs=20)\n",
        "    lstm_res_sub.append(l_acc_sub)\n",
        "\n",
        "plot_combined_results(dropout_list, lstm_res_domain, lstm_res_sub, \"Effect of Dropout\", \"Dropout Rate\", label_model_1=\"LSTM Val Acc Domain\", label_model_2=\"LSTM Val Acc Sub\")\n",
        "\n",
        "best_lstm_dropout_index_domain = np.argmax(lstm_res_domain)\n",
        "best_lstm_dropout_index_sub = np.argmax(lstm_res_sub)\n",
        "\n",
        "BEST_DROPOUT_LSTM_DOMAIN = dropout_list[best_lstm_dropout_index_domain]\n",
        "BEST_DROPOUT_LSTM_SUB = dropout_list[best_lstm_dropout_index_sub]"
      ],
      "metadata": {
        "id": "aK9oVlv1gObw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) test learning rate for LSTM (both classification tasks)\n",
        "lr_list = [1e-2, 1e-3, 1e-4]\n",
        "lstm_res_domain_lr = []\n",
        "lstm_res_sub_lr = []\n",
        "\n",
        "for lr in lr_list:\n",
        "    print(f\"testing learning rate: {lr}\")\n",
        "\n",
        "    l_acc_domain = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_domain,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_DOMAIN,\n",
        "        lr=lr\n",
        "    )\n",
        "    lstm_res_domain_lr.append(l_acc_domain)\n",
        "\n",
        "    l_acc_sub = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_sub,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_SUB,\n",
        "        lr=lr,\n",
        "        epochs=20\n",
        "    )\n",
        "    lstm_res_sub_lr.append(l_acc_sub)\n",
        "\n",
        "plot_combined_results(lr_list, lstm_res_domain_lr, lstm_res_sub_lr, \"Effect of Learning Rate\", \"Learning Rate\", label_model_1=\"LSTM Val Acc Domain\", label_model_2=\"LSTM Val Acc Sub\")\n",
        "\n",
        "best_lstm_lr_index_domain = np.argmax(lstm_res_domain_lr)\n",
        "best_lstm_lr_index_sub = np.argmax(lstm_res_sub_lr)\n",
        "\n",
        "BEST_LR_LSTM_DOMAIN = lr_list[best_lstm_lr_index_domain]\n",
        "BEST_LR_LSTM_SUB = lr_list[best_lstm_lr_index_sub]"
      ],
      "metadata": {
        "id": "tLYggLrOgP2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) test hidden size for LSTM (both classification tasks)\n",
        "hidden_size_list = [128, 256, 512]\n",
        "lstm_res_domain_hidden = []\n",
        "lstm_res_sub_hidden = []\n",
        "\n",
        "for hs in hidden_size_list:\n",
        "    print(f\"testing hidden size: {hs}\")\n",
        "\n",
        "    l_acc_domain = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_domain,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_DOMAIN,\n",
        "        lr=BEST_LR_LSTM_DOMAIN,\n",
        "        hidden_size=hs\n",
        "    )\n",
        "    lstm_res_domain_hidden.append(l_acc_domain)\n",
        "\n",
        "    l_acc_sub = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_sub,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_SUB,\n",
        "        lr=BEST_LR_LSTM_SUB,\n",
        "        hidden_size=hs,\n",
        "        epochs=20\n",
        "    )\n",
        "    lstm_res_sub_hidden.append(l_acc_sub)\n",
        "\n",
        "plot_combined_results(hidden_size_list, lstm_res_domain_hidden, lstm_res_sub_hidden, \"Effect of Hidden Size\", \"Hidden Size\", label_model_1=\"LSTM Val Acc Domain\", label_model_2=\"LSTM Val Acc Sub\")\n",
        "\n",
        "best_lstm_hs_index_domain = np.argmax(lstm_res_domain_hidden)\n",
        "best_lstm_hs_index_sub = np.argmax(lstm_res_sub_hidden)\n",
        "\n",
        "BEST_HS_LSTM_DOMAIN = hidden_size_list[best_lstm_hs_index_domain]\n",
        "BEST_HS_LSTM_SUB = hidden_size_list[best_lstm_hs_index_sub]"
      ],
      "metadata": {
        "id": "jHjRgFilgRB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) test init type for LSTM\n",
        "init_types = ['xavier', 'random', 'zero']\n",
        "init_results = []\n",
        "\n",
        "for hs in init_types:\n",
        "    print(f\"testing initialization type: {hs}\")\n",
        "\n",
        "    l_acc_domain = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_domain,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_DOMAIN,\n",
        "        lr=BEST_LR_LSTM_DOMAIN,\n",
        "        hidden_size=BEST_HS_LSTM_DOMAIN,\n",
        "    )\n",
        "    init_results.append(l_acc_domain)\n",
        "\n",
        "plot_single_result(init_types, init_results, \"Effect of Initialization\", \"Init Type\")"
      ],
      "metadata": {
        "id": "4KNzUGEHgSLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Test L1 Regularization for LSTM (Domain Task Only)\n",
        "# Testing: None (0.0), Small (1e-5), Moderate (1e-3)\n",
        "l1_values = [0.0, 1e-5, 1e-3]\n",
        "l1_results = []\n",
        "\n",
        "for l1_val in l1_values:\n",
        "    print(f\"testing L1 lambda: {l1_val}\")\n",
        "\n",
        "    l_acc_domain = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_domain,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_DOMAIN,\n",
        "        lr=BEST_LR_LSTM_DOMAIN,\n",
        "        hidden_size=BEST_HS_LSTM_DOMAIN,\n",
        "        l1_lambda=l1_val\n",
        "    )\n",
        "    l1_results.append(l_acc_domain)\n",
        "\n",
        "l1_labels = [str(v) for v in l1_values]\n",
        "plot_single_result(l1_labels, l1_results, \"Effect of L1 Regularization\", \"L1 Lambda\")"
      ],
      "metadata": {
        "id": "tM_KuazkgTd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Embedding Dimension Comparison (50d vs 100d vs 200d vs 300d)\n",
        "print(\"GloVe dimension comparison\")\n",
        "GLOVE_DIR = \"glove.6B\"\n",
        "glove_dims = [50, 100, 200, 300]\n",
        "dim_results = []\n",
        "\n",
        "for dim in glove_dims:\n",
        "    filename = f\"glove.6B.{dim}d.txt\"\n",
        "    path = os.path.join(GLOVE_DIR, filename)\n",
        "\n",
        "    print(f\"\\ntesting GloVe dimension: {dim}d\")\n",
        "\n",
        "    # load specific matrix for this dimension\n",
        "    # pass 'dim' so the parser knows the vector size\n",
        "    current_matrix = load_glove_matrix(path, vocab, embed_dim=dim)\n",
        "\n",
        "    acc = evaluate_lstm(\n",
        "        train_ds, val_ds, vocab_size, output_dim_domain,\n",
        "        embedding_matrix=current_matrix,\n",
        "        dropout=BEST_DROPOUT_LSTM_DOMAIN,\n",
        "        lr=BEST_LR_LSTM_DOMAIN,\n",
        "        hidden_size=BEST_HS_LSTM_DOMAIN,\n",
        "        epochs=10\n",
        "    )\n",
        "    dim_results.append(acc)"
      ],
      "metadata": {
        "id": "k2m84AaEgUzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_labels = [str(d) for d in glove_dims]\n",
        "plot_single_result(dim_labels, dim_results, \"Effect of Embedding Dimension\", \"GloVe Dimension\")"
      ],
      "metadata": {
        "id": "FnX9Z2togWZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3: REQUIRED CLASSIFICATION EXPERIMENTS\n",
        "# 1) custom LSTM (comain classification - 7 classes)\n",
        "print(\"\\n--- EXP 1/4: Custom LSTM (Domain Classification) ---\")\n",
        "lstm_domain = CustomLSTMModel(\n",
        "    vocab_size=len(vocab), embed_dim=EMBED_DIM, hidden_size=BEST_HS_LSTM_DOMAIN, output_dim=output_dim_domain,\n",
        "    embedding_matrix=embedding_matrix, dropout=BEST_DROPOUT_LSTM_DOMAIN\n",
        ")\n",
        "lstm_domain.fit(train_loader, val_loader, epochs=10)\n",
        "test_acc_domain_lstm = lstm_domain.evaluate_acc(test_loader)\n",
        "print(f\"Final LSTM Domain Test Accuracy: {test_acc_domain_lstm:.2f}%\")"
      ],
      "metadata": {
        "id": "yGQmNtsTgXU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) BERT Fine-Tuning (Domain Classification - 7 classes)\n",
        "print(\"\\n--- EXP 2/4: BERT Fine-Tuning (Domain Classification) ---\")\n",
        "bert_domain_model = BERTClassifier(output_dim=output_dim_domain)\n",
        "\n",
        "# create BERT loaders using full raw text data\n",
        "bert_domain_train_ldr = DataLoader(TensorDataset(\n",
        "    bert_tokenizer(bert_train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['input_ids'],\n",
        "    bert_tokenizer(bert_train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['attention_mask'],\n",
        "    torch.tensor(bert_train_labels)\n",
        "), batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "bert_domain_val_ldr = DataLoader(TensorDataset(\n",
        "    bert_tokenizer(bert_val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['input_ids'],\n",
        "    bert_tokenizer(bert_val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['attention_mask'],\n",
        "    torch.tensor(bert_val_labels)\n",
        "), batch_size=BATCH_SIZE)\n",
        "\n",
        "bert_domain_model.fit(bert_domain_train_ldr, bert_domain_val_ldr, epochs=3)\n",
        "\n",
        "# setup BERT test loader\n",
        "bert_test_loader, _, _, _ = get_bert_test_data(all_texts, y_domain_raw, test_indices)\n",
        "bert_test_loss, bert_test_acc = bert_domain_model.evaluate(bert_test_loader, nn.CrossEntropyLoss())\n",
        "test_acc_domain_bert = bert_test_acc * 100\n",
        "print(f\"Final BERT Domain Test Accuracy: {test_acc_domain_bert:.2f}%\")"
      ],
      "metadata": {
        "id": "GEzQt0apgY9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) custom LSTM (sub-field classification - 33 classes)\n",
        "print(\"\\n--- EXP 3/4: Custom LSTM (Sub-field Classification) ---\")\n",
        "lstm_sub = CustomLSTMModel(\n",
        "    vocab_size=len(vocab), embed_dim=EMBED_DIM, hidden_size=BEST_HS_LSTM_SUB, output_dim=output_dim_sub,\n",
        "    embedding_matrix=embedding_matrix, dropout=BEST_DROPOUT_LSTM_SUB\n",
        ")\n",
        "lstm_sub.fit(train_loader, val_loader, epochs=25)\n",
        "test_acc_sub_lstm = lstm_sub.evaluate_acc(test_loader)\n",
        "print(f\"Final LSTM Sub-field Test Accuracy: {test_acc_sub_lstm:.2f}%\")"
      ],
      "metadata": {
        "id": "6lgXNjlugaTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) BERT Fine-Tuning (sub-field classification - 33 classes)\n",
        "print(\"\\n--- EXP 4/4: BERT Fine-Tuning (Sub-field Classification) ---\")\n",
        "bert_sub_model = BERTClassifier(output_dim=output_dim_sub)\n",
        "\n",
        "# create BERT sub-field loaders\n",
        "bert_sub_train_ldr = DataLoader(TensorDataset(\n",
        "    bert_tokenizer(bert_train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['input_ids'],\n",
        "    bert_tokenizer(bert_train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['attention_mask'],\n",
        "    torch.tensor(bert_sub_train_labels)\n",
        "), batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "bert_sub_val_ldr = DataLoader(TensorDataset(\n",
        "    bert_tokenizer(bert_val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['input_ids'],\n",
        "    bert_tokenizer(bert_val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')['attention_mask'],\n",
        "    torch.tensor(bert_sub_val_labels)\n",
        "), batch_size=BATCH_SIZE)\n",
        "\n",
        "bert_sub_model.fit(bert_sub_train_ldr, bert_sub_val_ldr, epochs=3)\n",
        "\n",
        "# setup BERT test loader (Sub-field)\n",
        "bert_sub_test_loader, _, _, _ = get_bert_test_data(all_texts, y_sub_raw, test_indices)\n",
        "bert_sub_test_loss, bert_sub_test_acc = bert_sub_model.evaluate(bert_sub_test_loader, nn.CrossEntropyLoss())\n",
        "test_acc_sub_bert = bert_sub_test_acc * 100\n",
        "print(f\"Final BERT Sub-field Test Accuracy: {test_acc_sub_bert:.2f}%\")"
      ],
      "metadata": {
        "id": "9hWMPNh4gbMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_data(model, data_loader):\n",
        "    \"\"\"\n",
        "    Returns the input_ids, attention_mask, and tokenizer for one correct\n",
        "    and one incorrect prediction directly from the tensors.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_data = None\n",
        "    incorrect_data = None\n",
        "\n",
        "    # We need a tokenizer to decode the IDs back to text later\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            b_input_ids, b_input_mask, b_labels = [b.to(model.device) for b in batch]\n",
        "\n",
        "            logits = model(b_input_ids, b_input_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct_mask = preds == b_labels\n",
        "            incorrect_mask = preds != b_labels\n",
        "\n",
        "            # Capture one correct example\n",
        "            if correct_data is None and correct_mask.any():\n",
        "                idx = torch.where(correct_mask)[0][0]\n",
        "                # Store tuple: (input_ids, attention_mask)\n",
        "                # unsqueeze(0) keeps the batch dimension [1, seq_len]\n",
        "                correct_data = (b_input_ids[idx].unsqueeze(0), b_input_mask[idx].unsqueeze(0))\n",
        "\n",
        "            # Capture one incorrect example\n",
        "            if incorrect_data is None and incorrect_mask.any():\n",
        "                idx = torch.where(incorrect_mask)[0][0]\n",
        "                incorrect_data = (b_input_ids[idx].unsqueeze(0), b_input_mask[idx].unsqueeze(0))\n",
        "\n",
        "            if correct_data is not None and incorrect_data is not None:\n",
        "                break\n",
        "\n",
        "    return correct_data, incorrect_data, tokenizer"
      ],
      "metadata": {
        "id": "jsC6hEZagc4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention(model, tokenizer, input_ids, attention_mask, title_prefix, device, layer_idx=11, head_idx=0, save_dir=\"plots\"):\n",
        "    model.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    attentions = model.get_attention_maps(input_ids, attention_mask)\n",
        "    attention_matrix = attentions[layer_idx][0, head_idx, :, :].cpu().detach().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    # Clean tokens\n",
        "    seq_len = attention_mask.sum().item()\n",
        "    attention_matrix = attention_matrix[:seq_len, :seq_len]\n",
        "    tokens = tokens[:seq_len]\n",
        "\n",
        "    # --- ROBUST PLOTTING ---\n",
        "    # Create explicit Figure and Axes objects\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax)\n",
        "\n",
        "    title = f\"{title_prefix} - Layer {layer_idx+1}, Head {head_idx+1}\"\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Key (Attended To)\")\n",
        "    ax.set_ylabel(\"Query (Attending From)\")\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    clean_title = title.replace(\" \", \"_\").replace(\":\", \"\").replace(\",\", \"\")\n",
        "    filename = f\"{clean_title}.png\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    # Save the specific figure object, not the global state\n",
        "    fig.savefig(filepath, bbox_inches='tight')\n",
        "    plt.close(fig) # Close memory\n",
        "    print(f\"saved attention map to: {filepath}\")"
      ],
      "metadata": {
        "id": "hXGR-bAbge31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_token_importance(model, tokenizer, input_ids, attention_mask, title_prefix, device, layer_idx=11, head_idx=0, save_dir=\"plots\"):\n",
        "    \"\"\"\n",
        "    Plots a bar chart of the top 15 tokens that the [CLS] token attends to.\n",
        "    This solves the \"purple grid\" issue by auto-scaling the importance scores.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get attention maps\n",
        "    attentions = model.get_attention_maps(input_ids, attention_mask)\n",
        "\n",
        "    # Select specific layer and head\n",
        "    # Shape: [batch, num_heads, seq_len, seq_len]\n",
        "    # We want the attention FROM [CLS] (index 0) TO every other token\n",
        "    cls_attention = attentions[layer_idx][0, head_idx, 0, :].cpu().detach().numpy()\n",
        "\n",
        "    # Get tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    # Filter out [CLS], [SEP], [PAD] to see the actual content words\n",
        "    valid_tokens = []\n",
        "    valid_scores = []\n",
        "\n",
        "    for token, score in zip(tokens, cls_attention):\n",
        "        if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            valid_tokens.append(token)\n",
        "            valid_scores.append(score)\n",
        "\n",
        "    # Sort by score descending\n",
        "    sorted_indices = np.argsort(valid_scores)[::-1]\n",
        "    top_n = 15 # Only show top 15 for readability\n",
        "\n",
        "    top_tokens = [valid_tokens[i] for i in sorted_indices[:top_n]]\n",
        "    top_scores = [valid_scores[i] for i in sorted_indices[:top_n]]\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Horizontal bar chart\n",
        "    y_pos = np.arange(len(top_tokens))\n",
        "    ax.barh(y_pos, top_scores, align='center', color='skyblue')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(top_tokens)\n",
        "    ax.invert_yaxis()  # labels read top-to-bottom\n",
        "\n",
        "    title = f\"{title_prefix} Top Tokens - Layer {layer_idx+1} Head {head_idx+1}\"\n",
        "    ax.set_xlabel('Attention Score')\n",
        "    ax.set_title(title)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Save\n",
        "    clean_title = title.replace(\" \", \"_\").replace(\":\", \"\").replace(\"-\", \"_\")\n",
        "    filename = f\"{clean_title}.png\"\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "    fig.savefig(filepath, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved token importance plot to: {filepath}\")"
      ],
      "metadata": {
        "id": "p6gNCJKFgeyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3, REQUIREMENT 2: ATTENTION ANALYSIS\n",
        "# BERT domain model for attention analysis\n",
        "bert_test_loader_domain, _, _, _ = get_bert_test_data(all_texts, y_domain_raw, test_indices)\n",
        "\n",
        "correct_data, incorrect_data, tokenizer = get_sample_data(bert_domain_model, bert_test_loader_domain)"
      ],
      "metadata": {
        "id": "ceFm8mg9ghb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_attention(bert_domain_model, tokenizer, correct_data[0], correct_data[1], \"Correct\", DEVICE)\n",
        "visualize_attention(bert_domain_model, tokenizer, incorrect_data[0], incorrect_data[1], \"Incorrect\", DEVICE)"
      ],
      "metadata": {
        "id": "YA5adkThgicv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_token_importance(\n",
        "    bert_domain_model, tokenizer, correct_data[0], correct_data[1],\n",
        "    \"Correct\", DEVICE, layer_idx=11, head_idx=0\n",
        ")\n",
        "visualize_token_importance(\n",
        "    bert_domain_model, tokenizer, incorrect_data[0], incorrect_data[1],\n",
        "    \"Incorrect\", DEVICE, layer_idx=11, head_idx=0\n",
        ")"
      ],
      "metadata": {
        "id": "WvVdF06KgjVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- RESULTS SUMMARY TABLE ---\")\n",
        "print(\"| Model | Task | Test Accuracy | Winner? |\")\n",
        "print(\"|---|---|---|---|\")\n",
        "print(f\"| Custom LSTM (GloVe) | Domain (7 Classes) | {test_acc_domain_lstm:.2f}% | {'<--' if test_acc_domain_lstm > test_acc_domain_bert else ''} |\")\n",
        "print(f\"| BERT Classifier | Domain (7 Classes) | {test_acc_domain_bert:.2f}% | {'<--' if test_acc_domain_bert > test_acc_domain_lstm else ''} |\")\n",
        "print(f\"| Custom LSTM (GloVe) | Sub-field (33 Classes) | {test_acc_sub_lstm:.2f}% | {'<--' if test_acc_sub_lstm > test_acc_sub_bert else ''} |\")\n",
        "print(f\"| BERT Classifier | Sub-field (33 Classes) | {test_acc_sub_bert:.2f}% | {'<--' if test_acc_sub_bert > test_acc_sub_lstm else ''} |\")"
      ],
      "metadata": {
        "id": "rZLvkOZWgkgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_comparison(test_acc_domain_lstm, test_acc_domain_bert,\n",
        "                          test_acc_sub_lstm, test_acc_sub_bert,\n",
        "                          save_path=\"plots/model_comparison.png\"):\n",
        "\n",
        "    # Data preparation\n",
        "    tasks = ['Domain (7 Classes)', 'Sub-field (33 Classes)']\n",
        "    lstm_scores = [test_acc_domain_lstm, test_acc_sub_lstm]\n",
        "    bert_scores = [test_acc_domain_bert, test_acc_sub_bert]\n",
        "\n",
        "    x = np.arange(len(tasks))  # label locations\n",
        "    width = 0.35  # width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Plotting the bars\n",
        "    rects1 = ax.bar(x - width/2, lstm_scores, width, label='Custom LSTM (GloVe)', color='skyblue')\n",
        "    rects2 = ax.bar(x + width/2, bert_scores, width, label='BERT Classifier', color='salmon')\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_ylabel('Test Accuracy (%)')\n",
        "    ax.set_title('Model Performance Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(tasks)\n",
        "    ax.set_ylim(0, 100)  # Set y-axis to 0-100% for context\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Helper function to put text labels on top of bars\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.2f}%',\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"Comparison plot saved to {save_path}\")"
      ],
      "metadata": {
        "id": "DD1uaiXVglhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_comparison(test_acc_domain_lstm, test_acc_domain_bert, test_acc_sub_lstm, test_acc_sub_bert)"
      ],
      "metadata": {
        "id": "_DIsIxWsgm0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}